{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets Collection for FRP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sofia-1234567/Forensic-Research-Project-2021/blob/main/Tweets_Collection_for_FRP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HqXRWetX5w-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6358b36a-2ff5-4709-a235-bc0a12514c74"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql-M_aPkX8f-"
      },
      "source": [
        "#Twitter properties file contains the Twitter accesstoken, accesstokensecret,apikey and apisecretkey. \n",
        "!ls '/content/drive/My Drive/Colab Notebooks/twitterproperties.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLUtLiQvjYc5"
      },
      "source": [
        "#Tweepy is a Python library that allows us to access Twitter API. \n",
        "!pip install tweepy\n",
        "import tweepy as tw\n",
        "print(tw.__version__) #this will print the ver. of tweepy that has been imported\n",
        "\n",
        "#We will use the pandas library (a library for data manipulation and analysis) to build a dataframe for the collected tweets\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_mLanH4Ya78"
      },
      "source": [
        "#We will need the configparser package for reading the properties file from the Google Drive\n",
        "!pip install ConfigParser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYkheYmGYXjz"
      },
      "source": [
        "#In the properties file, the tokens and secret keys that will allow us to access the Twitter API are under a section called 'Twitter' \n",
        "import pandas as pd\n",
        "import configparser\n",
        "config = configparser.RawConfigParser()\n",
        "config.read('/content/drive/My Drive/Colab Notebooks/twitterproperties.txt')\n",
        "\n",
        "print(config.sections());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlqqBciFc5wQ"
      },
      "source": [
        "#Assign the contents of the Twitter section of the properties file to the the token and secret key variables. \n",
        "accesstoken=config.get('twitter', 'accesstoken')\n",
        "accesstokensecret=config.get('twitter', 'accesstokensecret')\n",
        "apikey=config.get('twitter', 'apikey')\n",
        "apisecretkey=config.get('twitter', 'apisecretkey')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NOYTahZfR5y"
      },
      "source": [
        "#Tweepy AuthHandler authorises the keys and tokens to be accessed \n",
        "auth = tw.OAuthHandler(apikey, apisecretkey)\n",
        "auth.set_access_token(accesstoken, accesstokensecret)\n",
        "\n",
        "#calling on auth and applying a wait limit so the server is not constantly pulling tweets (prevents crashing)\n",
        "#when the treshold of collected tweets is met, it will refresh in another 15 minutes to pull the next set of tweets\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gOksFCSfd8b"
      },
      "source": [
        "#searches for words and phrases (inside \"\") in one tweet using the OR and AND functions\n",
        "#search also includes queries for filtering out retweets, and filtering based on username and whether it includes an image\n",
        "search_words = '' '-filter:retweets AND from:user AND filter:twimg'\n",
        "\n",
        "#specify date we want the tweets from\n",
        "date_since = \"2020-01-01\"\n",
        "\n",
        "#tells the tweepy cursor we want to perform an API search of search_words, with language of the tweets being english, and to pull n tweets (items) at a time. \n",
        "tweets = tw.Cursor(api.search,\n",
        "                   q=search_words,\n",
        "                   lang=\"en\",\n",
        "                   since=date_since, tweet_mode='extended', include_entities=True).items(1000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q1tsbA7iTG8"
      },
      "source": [
        "#the items that will be extracted per tweet are: unique tweet identifier (as an integer), the full text of the tweet, the date tweet was created, and the URL to the image in attached to the tweet \n",
        "tweet_details = [[tweet.id, tweet.user.screen_name, tweet.full_text, tweet.created_at, tweet.entities['media'][0]['media_url']] for tweet in tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5Mjz9oPgnV7"
      },
      "source": [
        "#creating a dataframe of tweet_details\n",
        "#column headings are: \n",
        "tweet_df = pd.DataFrame(data=tweet_details, columns=['tweet_id','user', 'tweet_txt', 'created_at', 'tweet_img_URL'])\n",
        "\n",
        "#sets the maximum width in characters of a column. When the column overflows, a \"...\" placeholder is embedded in the output. \n",
        "pd.set_option('max_colwidth', 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2C7gvviQ9Oj"
      },
      "source": [
        "#filters the dataframe to only include rows containing the string \"pbs.twimg.com/media\" i.e. this will filter out entries linking to GIFs and video thumbnail\n",
        "tweet_df = tweet_df[tweet_df['tweet_img_URL'].str.contains(\"pbs.twimg.com/media\")]\n",
        "\n",
        "#this will show us the first 5 rows/entries in the dataframe\n",
        "tweet_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIUwNPBXVgIb"
      },
      "source": [
        "#this will give us the dimensions of the dataframe, i.e. how many rows and how many columns --> output is: (no. of rows, no. of columns)\n",
        "tweet_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zeN1Q4nHbv_"
      },
      "source": [
        "#dropping duplicate values in column \"tweet_txt\"\n",
        "\n",
        "#first, lets do some simple cleaning. Here, @ mentions are replaced with \"@user\", and https links are replaced with nothing (i.e. they are removed).\n",
        "import re\n",
        "def clean_tweets(Tweet):\n",
        "    Tweet = re.sub(\"@[\\w]*\",\"@user\",Tweet)\n",
        "    Tweet = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",Tweet)\n",
        "    return Tweet\n",
        "\n",
        "tweet_df['tweet_txt']=tweet_df['tweet_txt'].apply(lambda x: clean_tweets(x))\n",
        "\n",
        "#drop duplicate rows and keep the first instance\n",
        "tweet_df = tweet_df.drop_duplicates(subset=['tweet_txt'], keep='first')\n",
        "\n",
        "tweet_df.head(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Z-34ue1IhL"
      },
      "source": [
        "#let's see if any duplicates were removed, i.e. did the no. of rows decrease?\n",
        "tweet_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnoayUq4pbma"
      },
      "source": [
        "#saves the dataframe to .xlsx file without a column for row indexing\n",
        "#careful not to overwrite an existing file!\n",
        "#tweet_df.to_excel('BLM-6-neutral.xlsx', index=False)  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}